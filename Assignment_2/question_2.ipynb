{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0e+00 1.0e+00 2.2e+01 ... 0.0e+00 0.0e+00 0.0e+00]\n",
      " [0.0e+00 1.0e+00 2.2e+01 ... 0.0e+00 0.0e+00 0.0e+00]\n",
      " [0.0e+00 1.0e+00 2.2e+01 ... 0.0e+00 0.0e+00 0.0e+00]\n",
      " ...\n",
      " [0.0e+00 1.0e+00 2.2e+01 ... 1.0e-02 0.0e+00 0.0e+00]\n",
      " [0.0e+00 1.0e+00 2.2e+01 ... 1.0e-02 0.0e+00 0.0e+00]\n",
      " [0.0e+00 1.0e+00 2.2e+01 ... 1.0e-02 0.0e+00 0.0e+00]]\n",
      "494021 41\n",
      "(494021, 20)\n",
      "(494021, 20)\n",
      "(494021, 20)\n",
      "(494021, 20)\n",
      "(494021, 20)\n",
      "[3244311308453.997, 3244311308453.997, 3244311308453.997, 3244311308453.997, 3244311308453.997]\n",
      "[2.223191092525457e+17, 2.5846676191739357e+17, 2.8906780586699526e+17, 2.249260440405625e+17, 2.8535490993890925e+17]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "data = fetch_kddcup99()\n",
    "D = data.data\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "df = pd.DataFrame(D)\n",
    "\n",
    "categorical_features = [1, 2, 3]\n",
    "# print(\"Categorical Features:\", categorical_features)\n",
    "\n",
    "for col in categorical_features:\n",
    "    df[col] = encoder.fit_transform(df[col])\n",
    "\n",
    "D_encoded = df.to_numpy()\n",
    "\n",
    "D = D_encoded\n",
    "D = np.array(D, dtype=np.float64)\n",
    "\n",
    "print(D)\n",
    "\n",
    "n, d = D.shape\n",
    "print(n, d)\n",
    "\n",
    "k = 15\n",
    "reduced_dim = 20\n",
    "\n",
    "def compute_loss(data, centroids):\n",
    "    loss = 0\n",
    "    for point in data:\n",
    "        nearest_centroid = centroids[np.argmin(np.linalg.norm(point - centroids, axis=1))]\n",
    "        loss += np.sum((point - nearest_centroid) ** 2)\n",
    "    return loss\n",
    "\n",
    "losses_original = []\n",
    "losses_reduced = []\n",
    "\n",
    "for trial in range(5):\n",
    "    # jl_transformer = GaussianRandomProjection(n_components=reduced_dim)\n",
    "    # E = jl_transformer.fit_transform(D_numeric)\n",
    "\n",
    "    M = np.random.normal(loc=0.0, scale=1/np.sqrt(reduced_dim), size=(d, reduced_dim))\n",
    "    E = np.dot(D, M)\n",
    "\n",
    "    print(E.shape)\n",
    "\n",
    "    kmeans_reduced = KMeans(n_clusters=k, random_state=42).fit(E)\n",
    "    A = kmeans_reduced.cluster_centers_\n",
    "    # labels_reduced = kmeans_reduced.labels_\n",
    "    A_projected = np.dot(A, np.linalg.pinv(M))\n",
    "    \n",
    "    kmeans_original = KMeans(n_clusters=k, random_state=42).fit(D)\n",
    "    B = kmeans_original.cluster_centers_\n",
    "    # labels_original = kmeans_original.labels_\n",
    "    \n",
    "    \n",
    "    loss_reduced = compute_loss(D, A_projected)\n",
    "    loss_original = compute_loss(D, B)\n",
    "    \n",
    "    losses_reduced.append(loss_reduced)\n",
    "    losses_original.append(loss_original)\n",
    "\n",
    "\n",
    "print(losses_original)\n",
    "print(losses_reduced)\n",
    "\n",
    "# trials = range(1, 6)\n",
    "# plt.bar(trials, losses_original, width=0.4, label='Original Data Loss')\n",
    "# plt.bar([t + 0.4 for t in trials], losses_reduced, width=0.4, label='Reduced Data Loss')\n",
    "# plt.xlabel('Trial')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xticks([t + 0.2 for t in trials], labels=trials)\n",
    "# plt.legend()\n",
    "# plt.title('Comparison of Clustering Losses')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[58.47988423603355, 58.47988423603355, 58.47988423603355, 58.47988423603355, 58.47988423603355]\n",
      "[27490.476127654023, 30.90236745401269, 40.78288976264282, 46.407715437351904, 27.09586344940576]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Fetch the dataset\n",
    "data = fetch_kddcup99()\n",
    "D = data.data\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "df = pd.DataFrame(D)\n",
    "\n",
    "categorical_features = [1, 2, 3]\n",
    "# print(\"Categorical Features:\", categorical_features)\n",
    "\n",
    "for col in categorical_features:\n",
    "    df[col] = encoder.fit_transform(df[col])\n",
    "\n",
    "D_encoded = df.to_numpy()\n",
    "\n",
    "D = D_encoded\n",
    "D = np.array(D, dtype=np.float64)\n",
    "\n",
    "y= data.target\n",
    "#label encoding \n",
    "y = encoder.fit_transform(y)\n",
    "y = np.array(y, dtype=np.float64)\n",
    "\n",
    "# Get dimensions\n",
    "n, d = D.shape\n",
    "\n",
    "# Define JL Matrix Generator\n",
    "def generate_sparse_jl_matrix(rows, cols, sparsity=0.1):\n",
    "    return sparse_random(rows, cols, density=sparsity, data_rvs=np.random.randn).toarray()\n",
    "\n",
    "losses_original=[]\n",
    "losses_reduced=[]\n",
    "\n",
    "for _ in range(5):\n",
    "    # Generate sparse JL matrix\n",
    "    M = generate_sparse_jl_matrix(10, n)\n",
    "\n",
    "    # Compute projections\n",
    "    E = M @ D\n",
    "    z = M @ y\n",
    "\n",
    "    # Linear regression on original data\n",
    "    reg_original = LinearRegression().fit(D, y)\n",
    "    b = reg_original.coef_\n",
    "\n",
    "    # Linear regression on JL-projected data\n",
    "    reg_jl = LinearRegression().fit(E, z)\n",
    "    a = reg_jl.coef_\n",
    "\n",
    "    # Compute losses\n",
    "    loss_a = mean_squared_error(y, D @ a)\n",
    "    loss_b = mean_squared_error(y, D @ b)\n",
    "\n",
    "    losses_original.append(loss_b)\n",
    "    losses_reduced.append(loss_a)\n",
    "\n",
    "\n",
    "print(losses_original)\n",
    "print(losses_reduced)\n",
    "\n",
    "# Visualize results\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Prepare results for visualization\n",
    "# df_results = pd.DataFrame(loss_results)\n",
    "# df_results.index.name = 'Trial'\n",
    "\n",
    "# # Plot results\n",
    "# df_results.plot(kind='bar', figsize=(10, 6))\n",
    "# plt.title('Loss Comparison for Original and JL-Projected Data')\n",
    "# plt.xlabel('Trial')\n",
    "# plt.ylabel('Mean Squared Error')\n",
    "# plt.legend(['Loss (a)', 'Loss (b)'])\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
